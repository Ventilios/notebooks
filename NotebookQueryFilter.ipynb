{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Connect to Synapse using the official route"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.DataFrame\r\n",
        "import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
        "\r\n",
        "val dfToReadFromTable:DataFrame = spark.read.\r\n",
        "    option(Constants.SERVER, \"<server>.sql.azuresynapse.net,1433\").\r\n",
        "    synapsesql(\"<db>.dbo.trip\").\r\n",
        "    select(\"DateID\", \"PaymentType\", \"FareAmount\"). \r\n",
        "    filter(col(\"PaymentType\") === \"CSH\").\r\n",
        "    limit(10)\r\n",
        "\r\n",
        "dfToReadFromTable.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Connect to Synapse using JDBC and SQL authentication"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
        "import java.util.Properties\r\n",
        "\r\n",
        "// Change values towards your environment\r\n",
        "// Store the values in key vault.\r\n",
        "val jdbcHostname = \"<server>.sql.azuresynapse.net\"\r\n",
        "val jdbcPort = 1433\r\n",
        "val jdbcDatabase = \"<database>\"\r\n",
        "\r\n",
        "// Create the JDBC URL without passing in the user and password parameters.\r\n",
        "val jdbcUrl = s\"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase}\"\r\n",
        "\r\n",
        "// Create a Properties() object to hold the parameters.\r\n",
        "val connectionProperties = new Properties()\r\n",
        "\r\n",
        "// Driver that can also be observed in the log when using the offical Synapse SQL connection route\r\n",
        "val driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "connectionProperties.setProperty(\"Driver\", driverClass)\r\n",
        "connectionProperties.setProperty(\"authentication\", \"SqlPassword\")\r\n",
        "\r\n",
        "// Store secrets in keyvault\r\n",
        "// https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-secure-credentials-with-tokenlibrary?pivots=programming-language-scala#getsecret\r\n",
        "connectionProperties.setProperty(\"username\", TokenLibrary.getSecret(\"<key-vault-name>\", \"<property-containing-username>\") )\r\n",
        "connectionProperties.setProperty(\"password\", TokenLibrary.getSecret(\"<key-vault-name>\", \"<property-containing-password>\"))\r\n",
        "\r\n",
        "// Set isolation level\r\n",
        "// https://docs.microsoft.com/en-us/sql/connect/jdbc/understanding-isolation-levels?view=sql-server-ver15\r\n",
        "connectionProperties.setProperty(\"setTransactionIsolation\", \"TRANSACTION_READ_UNCOMMITTED\")\r\n",
        "\r\n",
        "// Define your query\r\n",
        "val pushdown_query = \"(select top 10 DateID, PaymentType, FareAmount from dbo.trip where paymenttype = 'CSH') data_alias\"\r\n",
        "val df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n",
        "display(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Connect to Synapse using JDBC and Managed Identity"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
        "import java.util.Properties\r\n",
        "\r\n",
        "val jdbcHostname = \"<server>.sql.azuresynapse.net\"\r\n",
        "val jdbcPort = 1433\r\n",
        "val jdbcDatabase = \"<database>\"\r\n",
        "\r\n",
        "// Create the JDBC URL without passing in the user and password parameters.\r\n",
        "val jdbcUrl = s\"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase}\"\r\n",
        "\r\n",
        "// Create a Properties() object to hold the parameters.\r\n",
        "val connectionProperties = new Properties()\r\n",
        "\r\n",
        "// Driver that can also be observed in the log when using the 'native' SynapseSQL way.\r\n",
        "val driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "connectionProperties.setProperty(\"Driver\", driverClass)\r\n",
        "\r\n",
        "// Create a linked server to your dedicated pool with a Manged Identity\r\n",
        "// Add linked server name to the getConnectionStringOrCreds function\r\n",
        "connectionProperties.setProperty(\"accessToken\", mssparkutils.credentials.getConnectionStringOrCreds(\"<linked-server-to-synapse\"))\r\n",
        "\r\n",
        "// Set isolation level\r\n",
        "// https://docs.microsoft.com/en-us/sql/connect/jdbc/understanding-isolation-levels?view=sql-server-ver15\r\n",
        "connectionProperties.setProperty(\"setTransactionIsolation\", \"TRANSACTION_READ_UNCOMMITTED\")\r\n",
        "\r\n",
        "// Define your query\r\n",
        "val pushdown_query = \"(select top 10 DateID, PaymentType, FareAmount from dbo.trip where paymenttype = 'CSH') data_alias\"\r\n",
        "val df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n",
        "display(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "scala"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
